{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load COCO Dataset from FiftyOne\n",
        "\n",
        "Import the MS COCO dataset using FiftyOne's dataset zoo into Pixeltable tables.\n",
        "\n",
        "**What's in this recipe:**\n",
        "- Import COCO dataset with images and annotations using FiftyOne\n",
        "- Sample 1% of the training split (~1,183 images from 118,287 total)\n",
        "- Automatic schema handling for images and labels\n",
        "- Work with image-detection pairs in Pixeltable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n",
        "\n",
        "MS COCO is a large-scale object detection and segmentation dataset with 118,287 training images. You need a representative sample of this dataset in Pixeltable to apply AI models, create embeddings, or run analysis without downloading the entire dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution\n",
        "\n",
        "**What's in this recipe:**\n",
        "- Import 1% sample (~1,183 images) from COCO-2017 training split using FiftyOne\n",
        "- Automatic schema handling for images and detection labels\n",
        "- Work with image-detection pairs and metadata in Pixeltable\n",
        "\n",
        "You can use FiftyOne's dataset zoo to efficiently download specific subsets of COCO, then import them into Pixeltable tables. This allows you to work with exactly the data you need without downloading the entire dataset.\n",
        "\n",
        "You can iterate on transformations before adding them to your table. Use `.select()` with `.collect()` to preview results on sample dataâ€”nothing is stored in your table. If you want to collect only the first few rows, use `.head(n)` instead of `.collect()`. Once you're satisfied, use `.add_computed_column()` to apply transformations to all rows in your table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mResolved \u001b[1m275 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m178 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv add pixeltable fiftyone transformers torch accelerate pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pixeltable as pxt\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pxt.list_tables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load COCO Dataset from FiftyOne\n",
        "\n",
        "Load the [COCO-2017 dataset](https://docs.voxel51.com/dataset_zoo/datasets/coco_2017.html) from FiftyOne's dataset zoo. We'll download 1,183 random samples from the training split (1% of 118,287 total training images).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load 1,183 random samples from COCO-2017 training split (1% of 118,287)\n",
        "# FiftyOne only downloads the specific images needed\n",
        "coco_dataset = foz.load_zoo_dataset(\n",
        "    'coco-2017',\n",
        "    split='train',\n",
        "    max_samples=1183,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directory for COCO data\n",
        "pxt.drop_dir('coco_images', force=True)\n",
        "pxt.create_dir('coco_images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Pixeltable Table\n",
        "\n",
        "Now create a table and insert the sampled data. Each row contains an image with its associated captions and metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create table with schema for images and labels\n",
        "t = pxt.create_table(\n",
        "    'coco_images.samples',\n",
        "    schema={\n",
        "        'image': pxt.Image,\n",
        "        'coco_id': pxt.Int,\n",
        "        'num_detections': pxt.Int\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare rows for insertion from FiftyOne dataset\n",
        "rows = []\n",
        "for idx, sample in enumerate(coco_dataset):\n",
        "    # Check available fields and extract detection count\n",
        "    # FiftyOne stores detections in 'ground_truth' field for COCO\n",
        "    num_dets = 0\n",
        "    if hasattr(sample, 'ground_truth') and sample.ground_truth:\n",
        "        num_dets = len(sample.ground_truth.detections)\n",
        "    \n",
        "    rows.append({\n",
        "        'image': sample.filepath,\n",
        "        'coco_id': idx,\n",
        "        'num_detections': num_dets\n",
        "    })\n",
        "\n",
        "t.insert(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View sample data\n",
        "t.select(t.image, t.coco_id, t.num_detections).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check total count\n",
        "t.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract Image Metadata\n",
        "\n",
        "Add computed columns to extract metadata from the images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add computed columns for image dimensions\n",
        "t.add_computed_column(width=t.image.width)\n",
        "t.add_computed_column(height=t.image.height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View images with their dimensions and detection counts\n",
        "t.select(t.image, t.num_detections, t.width, t.height).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add CLIP Embeddings for Image Search\n",
        "\n",
        "Create vector embeddings for the images using OpenAI's CLIP model. These embeddings enable semantic image search and similarity comparisons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/alison-pxt/.pixeltable/pgdata\n"
          ]
        }
      ],
      "source": [
        "#t = pxt.get_table('coco_images.samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "# Add image embeddings using HuggingFace CLIP model\n",
        "from pixeltable.functions.huggingface import clip\n",
        "\n",
        "# Use the correct HuggingFace model ID format\n",
        "t.add_embedding_index(\n",
        "    'image',\n",
        "    embedding=clip.using(model_id='openai/clip-vit-base-patch16')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Image Captions with BLIP\n",
        "\n",
        "Use BLIP (Bootstrapping Language-Image Pre-training), an efficient open-source image captioning model from Salesforce. BLIP generates natural, descriptive captions and runs locally without API keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alison-pxt/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0f4091491ff4deaa775efaf068d966d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07eb62006fa840c59dc5a42615667dc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aba8c456dcd3495a874d420555036691",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2116c0ae45542efaca18fc85ed75cb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "691384a36abb49a1bfbf8c9c8d8c73df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56e5e3276e144610946c94a820af3ef7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa871d2e5625439ab4a446766049529e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f674716408a4a639a6faba6e231c059",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 1183 column values with 0 errors.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1183 rows updated, 2366 values computed."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate image captions using BLIP\n",
        "from pixeltable.functions.huggingface import image_captioning\n",
        "\n",
        "t.add_computed_column(\n",
        "    caption=image_captioning(t.image, model_id='Salesforce/blip-image-captioning-base')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "Error",
          "evalue": "Exception in task: 'tokens'\nTraceback (most recent call last):\n  File \"/Users/alison-pxt/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/exec/expr_eval/expr_eval_node.py\", line 396, in _done_cb\n    t.result()\n  File \"/Users/alison-pxt/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/futures.py\", line 203, in result\n    raise self._exception.with_traceback(self._exception_tb)\n  File \"/Users/alison-pxt/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n  File \"/Users/alison-pxt/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/exec/expr_eval/schedulers.py\", line 138, in _main_loop\n    last_report_ts = self.pool_info.resource_limits[limits_info.resource].recorded_at\n                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\nKeyError: 'tokens'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Add OpenAI GPT-4o-mini captions for comparison\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpixeltable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m openai\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_computed_column\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopenai_caption\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvision\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDescribe this image in one sentence, focusing on the main objects, their actions, and the setting. Use clear, factual language similar to COCO dataset captions.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/catalog/table.py:717\u001b[39m, in \u001b[36mTable.add_computed_column\u001b[39m\u001b[34m(self, stored, destination, print_stats, on_error, if_exists, **kwargs)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;28mself\u001b[39m._verify_column(new_col)\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tbl_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m result += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tbl_version\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m FileCache.get().emit_eviction_warnings()\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/catalog/table_version.py:802\u001b[39m, in \u001b[36mTableVersion.add_columns\u001b[39m\u001b[34m(self, cols, print_stats, on_error)\u001b[39m\n\u001b[32m    800\u001b[39m         all_cols.append(undo_col)\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# Add all columns\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m status = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[38;5;66;03m# Create indices and their md records\u001b[39;00m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col, (idx, val_col, undo_col) \u001b[38;5;129;01min\u001b[39;00m index_cols.items():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/catalog/table_version.py:868\u001b[39m, in \u001b[36mTableVersion._add_columns\u001b[39m\u001b[34m(self, cols, print_stats, on_error)\u001b[39m\n\u001b[32m    866\u001b[39m plan.open()\n\u001b[32m    867\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m     excs_per_col = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore_tbl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mabort\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m sql_exc.DBAPIError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    870\u001b[39m     Catalog.get().convert_sql_exc(exc, \u001b[38;5;28mself\u001b[39m.id, \u001b[38;5;28mself\u001b[39m.handle, convert_db_excs=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/store.py:319\u001b[39m, in \u001b[36mStoreBase.load_column\u001b[39m\u001b[34m(self, col, exec_plan, abort_on_exc)\u001b[39m\n\u001b[32m    316\u001b[39m table_rows: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[Any]] = []\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# insert rows from exec_plan into temp table\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexec_plan\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_table_rows\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/exec/exec_node.py:58\u001b[39m, in \u001b[36mExecNode.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m         batch: DataRowBatch = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43maiter\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__anext__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m batch\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/exec/expr_eval/expr_eval_node.py:289\u001b[39m, in \u001b[36mExprEvalNode.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.error \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mself\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merror\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.error\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m completed_aw \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_state(\u001b[33m'\u001b[39m\u001b[33mcompleted_aw done\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mError\u001b[39m: Exception in task: 'tokens'\nTraceback (most recent call last):\n  File \"/Users/alison-pxt/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/exec/expr_eval/expr_eval_node.py\", line 396, in _done_cb\n    t.result()\n  File \"/Users/alison-pxt/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/futures.py\", line 203, in result\n    raise self._exception.with_traceback(self._exception_tb)\n  File \"/Users/alison-pxt/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n  File \"/Users/alison-pxt/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/exec/expr_eval/schedulers.py\", line 138, in _main_loop\n    last_report_ts = self.pool_info.resource_limits[limits_info.resource].recorded_at\n                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\nKeyError: 'tokens'\n"
          ]
        }
      ],
      "source": [
        "# Add OpenAI GPT-4o-mini captions for comparison\n",
        "from pixeltable.functions import openai\n",
        "\n",
        "t.add_computed_column(\n",
        "    openai_caption=openai.vision(\n",
        "        prompt=\"Describe this image in one sentence, focusing on the main objects, their actions, and the setting. Use clear, factual language similar to COCO dataset captions.\",\n",
        "        image=t.image,\n",
        "        model='gpt-4o-mini'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "Unknown column: openai_caption",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compare BLIP and OpenAI captions side-by-side\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m t.select(t.image, t.caption, \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopenai_caption\u001b[49m, t.num_detections).head(\u001b[32m5\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/pxt-cloud-sets/.venv/lib/python3.11/site-packages/pixeltable/catalog/table.py:193\u001b[39m, in \u001b[36mTable.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    191\u001b[39m col = \u001b[38;5;28mself\u001b[39m._tbl_version_path.get_column(name)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnknown column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ColumnRef(col, reference_tbl=\u001b[38;5;28mself\u001b[39m._tbl_version_path)\n",
            "\u001b[31mAttributeError\u001b[39m: Unknown column: openai_caption"
          ]
        }
      ],
      "source": [
        "# Compare BLIP and OpenAI captions side-by-side\n",
        "t.select(t.image, t.caption, t.openai_caption, t.num_detections).head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View images with their AI-generated captions from InternVL3\n",
        "t.select(t.image, t.internvl_caption, t.num_detections).head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Publish to Pixeltable Cloud\n",
        "\n",
        "Publish the table to make it available on Pixeltable Cloud.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Publish the table to Pixeltable Cloud\n",
        "pxt.publish(\n",
        "    t,\n",
        "    'pxt://pixeltable:fiftyone/coco_mini_2017',\n",
        "    access='public'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.push()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.drop_column('filepath')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.push()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (pxt-cloud-sets)",
      "language": "python",
      "name": "pxt-cloud-sets"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
